# grinder.py

"""Tools for working with Grinder log output and generating CSV data.

This module defines three important classes:

    - `Bin`: Statistics collected over a certain time interval
    - `Test`: All statistics for a single Test in a Grinder test run
    - `Report`: Collection of Test statistics and functions to write CSV reports

To generate reports, simply instantiate a `Report` instance, providing the
report granularity in seconds, the name of the ``out_*`` file, and at least one
``data_*`` file generated by Grinder::

    from csvsee import grinder
    report = grinder.Report(60, 'out-0.log', 'data-0.log')

Or, if you have multiple ``data_*`` files::

    report = grinder.Report(60, 'out-0.log', 'data-0.log', 'data-1.log', 'data-2.log')

The granularity determines how fine-grained the timestamps in your report will
be; with 60-second granularity, all statistics during each 60-second interval
are accumulated and reported together. For more detail, you could use 1-second
granularity::

    report = grinder.Report(1, 'out-0.log', 'data-0.log')

but this will increase the size of your CSV data considerably and result in
much noisier-looking data. If you're doing a long-term load test spanning
hours, you might use a larger value, say 10 minutes::

    report = grinder.Report(600, 'out-0.log', 'data-0.log')

This will give smaller CSV files with smoother data, at the expense of some
detail; you won't be able to see data spikes as easily.

Now, you can generate a bunch of predetermined CSV files like this::

    report.write_all_csvs('my_results')

The given string will be prefixed all the CSV filenames.
"""

import os
import shlex
import csv
import re
from glob import glob
from datetime import datetime


def get_test_names(outfile):
    """Return a dict of ``{number: name}`` for each test from the summary
    portion of the given Grinder ``out*`` file. If the summary portion is
    not found, look for test numbers and names as logged by grinder-webtest.
    """
    summary_tests = {}
    webtest_tests = {}
    for line in open(outfile, 'r'):
        # Look for summary lines
        if line.lstrip('(').startswith('Test '):
            fields = shlex.split(line)
            number, name = int(fields[1]), fields[-1]
            summary_tests[number] = name
        # Look for grinder-webtest lines
        elif '------' in line:
            m = re.match('^.*: ------ Test (\d+): (.+)$', line)
            if m:
                number, name = m.groups()
                webtest_tests[int(number)] = name

    if summary_tests:
        return summary_tests
    else:
        return webtest_tests


class NoTestNames (Exception):
    """Failure to find any test names in a Grinder ``out*`` file.
    """
    pass


class Bin:
    """Accumulated statistics for an interval of time.
    """
    def __init__(self, stat_names):
        """Create a bin for accumulating the given statistics.

            >>> b = Bin(['Errors', 'HTTP response length', 'Test time'])

        """
        self.stats = dict((stat, 0) for stat in stat_names)
        self.count = 0


    def add(self, row):
        """Accumulate a row of statistics in this bin.
        All statistics are accumulated as integers.
        """
        for stat in self.stats:
            # FIXME: Tally up all HTTP response codes instead of only checking
            # for this one (since it makes no sense to add numeric HTTP
            # response codes together)
            if stat == '503 Errors':
                if row['HTTP response code'].find('503') >= 0:
                    self.stats[stat] += 1
            else:
                self.stats[stat] += int(row[stat])
        self.count += 1


    def average(self, stat):
        """Return the integer average (mean) of the given statistic.
        """
        if self.count > 0:
            return (self.stats[stat] / self.count)
        else:
            return 0


class Test:
    """Statistics for a single Test in a Grinder test run.
    """
    # Statistics to sum
    sum_stats = [
        'Errors',
        #'HTTP response errors',
        'HTTP response code',
        '503 Errors',
    ]
    # Statistics to average
    average_stats = [
        'HTTP response length',
        'Test time',
        # May need these, or may not...
        #'Time to establish connection',
        #'Time to first byte',
        #'Time to resolve host',
    ]
    all_stats = sum_stats + average_stats


    def __init__(self, number, name, granularity=1):
        """Create a Test with the given number and name, and a granularity in
        seconds.
        """
        self.number = number
        self.name = name
        self.granularity = granularity
        # Bins of accumulated statistics, indexed by timestamp in seconds
        self.bins = {}


    def add(self, row):
        """Add a row of statistics for this test.
        """
        # Convert timestamp to seconds
        timestamp = int(row['Start time (ms since Epoch)']) / 1000
        # Truncate the timestamp to the current granularity
        if self.granularity > 1:
            timestamp = (timestamp / self.granularity) * self.granularity
        # If a bin doesn't exist for this timestamp, create one
        if timestamp not in self.bins:
            self.bins[timestamp] = Bin(Test.all_stats)
        # Accumulate stats
        self.bins[timestamp].add(row)


    def timestamp_range(self):
        """Return the ``(start, end)`` timestamps for this test.
        """
        if not self.bins:
            return (0, 0)
        timestamps = sorted(self.bins.keys())
        return (min(timestamps), max(timestamps))


    def stat_at_time(self, stat, timestamp):
        """Return a statistic at the given timestamp (either sum or average).
        Return ``0`` if there is no data at the given time.
        """
        if timestamp not in self.bins:
            return 0
        # Get the appropriate bin
        bin = self.bins[timestamp]
        # For summed stats, just return the total
        if stat in Test.sum_stats:
            return bin.stats[stat]
        # For averaged stats, divide by count
        elif stat in Test.average_stats:
            return (bin.stats[stat] / bin.count)
        # Special handling for transaction count
        elif stat in ['transactions', 'transactions-page-requests']:
            return bin.count
        elif stat == 'Test time-page-requests':
            return (bin.stats['Test time'] / bin.count)
        else:
            raise ValueError("Unknown stat: %s" % stat)


    def __str__(self):
        return "%s: %s" % (self.number, self.name)


class Report:
    """A report of statistics for a Grinder test run.
    """
    def __init__(self, granularity, grinder_outfile, *grinder_datafiles):
        self.granularity = granularity
        self.outfile = grinder_outfile
        self.datafiles = grinder_datafiles
        self.tests = {}
        self.populate_stats()


    def populate_stats(self):
        """Add statistics for all tests in all Grinder data files.
        """
        # Get test (number, name) pairs
        for (number, name) in get_test_names(self.outfile).iteritems():
            self.tests[number] = Test(number, name, self.granularity)

        if not self.tests:
            raise NoTestNames("No test names found in '%s'" % self.outfile)

        for datafile in self.datafiles:
            print("Getting test stats from %s" % datafile)
            data = csv.DictReader(open(datafile, 'r'), skipinitialspace=True)
            for row in data:
                self.add(row)


    def add(self, row):
        """Add a row from a ``data*`` file to the stats.
        """
        test_num = int(row['Test'])
        # Get the Test for this test number
        try:
            test = self.tests[test_num]
        # If this is an unknown test number, ignore it
        except KeyError:
            pass
        # Otherwise, add the row to the test stats
        else:
            test.add(row)


    def timestamp_range(self):
        """Return the ``(start, end)`` timestamps for this report, based
        on the timestamps of all tests within it.
        """
        # Get all (start, end) ranges from the tests
        ranges = [test.timestamp_range() for test in self.tests.values()]
        # Using list() here to future-proof
        start_times, end_times = list(zip(*ranges))
        return (min(start_times), max(end_times))


    def write_csv(self, stat, filename):
        """Write the given statistic for all tests to ``filename``.
        """
        # Open the CSV file for writing
        csv_writer = csv.writer(open(filename, 'w'))

        # Test number determines the order of columns
        test_numbers = sorted(self.tests.keys())

        # Certain kinds of stat need certain test numbers filtered out.
        # For page-requests, only include test numbers ending in 00
        if stat in ['transactions-page-requests', 'Test time-page-requests']:
            test_numbers = [n for n in test_numbers if n % 100 == 0]
        # For transactions/test-time, only include those NOT ending in 00
        elif stat in ['transactions', 'Test time']:
            test_numbers = [n for n in test_numbers if n % 100 > 0]
        # For all other stats, include all test numbers
        else:
            pass

        # OOCalc has a hard limit of 65535 characters in a single line of a
        # .csv file. Figure out where to truncate the test names so they will
        # all fit in the header row.
        trunc_length = 65000 / len(test_numbers)

        # Assemble the header row
        header = ['GMT']
        # Sort by test number
        for test_num in test_numbers:
            trunc_name = str(self.tests[test_num])[:trunc_length]
            header.append(trunc_name)
        # Write the header row
        csv_writer.writerow(header)

        # Assemble and write each row, sorted by timestamp
        start_time, end_time = self.timestamp_range()
        this_time = start_time
        while this_time <= end_time:
            timestamp = datetime.utcfromtimestamp(this_time)
            timestamp = datetime.strftime(timestamp, '%m/%d/%Y %H:%M:%S') + '.000'
            row = [timestamp]
            for test_num in test_numbers:
                row.append(self.tests[test_num].stat_at_time(stat, this_time))
            # Write the row
            csv_writer.writerow(row)
            # Step to the next timestamp
            this_time += self.granularity


    def write_all_csvs(self, csv_prefix):
        """Write all CSV files for this report to files with the given prefix.
        """
        # Specific stats
        for stat in Test.all_stats:
            csv_filename = "%s_%s.csv" % (csv_prefix, stat.replace(' ', '_'))
            print("Writing %s" % csv_filename)
            self.write_csv(stat, csv_filename)

        # Transaction counts
        csv_filename = "%s_Transaction_count.csv" % csv_prefix
        print("Writing %s" % csv_filename)
        self.write_csv('transactions', csv_filename)

        # Transaction counts - page requests
        csv_filename = "%s_Transaction_count_page_requests_only.csv" % csv_prefix
        print("Writing %s" % csv_filename)
        self.write_csv('transactions-page-requests', csv_filename)

        # Test time - page requests
        csv_filename = "%s_Test-time_page_requests_only.csv" % csv_prefix
        print("Writing %s" % csv_filename)
        self.write_csv('Test time-page-requests', csv_filename)



def grinder_files(include_dir):
    """Return a list of full pathnames to all ``out*`` and ``data*`` files
    found in descendants of ``include_dir``.
    """
    if not os.path.exists(include_dir):
        raise ValueError("No such directory: %s" % include_dir)

    out_data_files = []
    for (path, dirs, files) in os.walk(include_dir):
        outfiles = glob(os.path.join(path, 'out_*.log'))
        datafiles = sorted(glob(os.path.join(path, 'data_*.log')))
        if outfiles and datafiles:
            out_data_files.append((outfiles[0], datafiles))

    return out_data_files



